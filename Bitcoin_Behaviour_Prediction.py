# -*- coding: utf-8 -*-
"""MIDTERM_PROJECT_FINAL

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ocOuATYHkjr-_W0vyYejEDPtunv-WjJ5
"""

#import the libraries that are going to be used
import pandas as pd
import numpy as np
import yfinance as yf
from matplotlib import pyplot as plt
import seaborn as sns
import plotly.graph_objects as pg
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix


#Fetch Data (add votality index, us dollar index and us 10 year treasury yield in addition to BTC/USD and S&P500)
tickers = {
    'BTC-USD': 'BTC',
    '^GSPC': 'SP500',
    '^VIX': 'VIX',       # Volatility Index (Fear Gauge)
    'DX-Y.NYB': 'DXY',   # US Dollar Index
    '^TNX': 'TNX',       # US 10-Year Treasury Yield
    '^NDX': 'NASDAQ100',
}

#define start and end dates
start_date = '2015-01-01'
end_date = '2025-01-01'

# Download Closing prices
raw_data = yf.download(list(tickers.keys()), start=start_date, end=end_date)['Close']

# Rename columns
raw_data = raw_data.rename(columns=tickers)

# Forward fill missing data
df = raw_data.ffill()
df = df.dropna()

print(f"Data fetched and cleaned. Shape: {df.shape}")
display(df.tail())

merged_df = df.copy()

# Fetch BTC-USD full data separately to get 'Volume'
btc_full = yf.download('BTC-USD', start=start_date, end=end_date)
merged_df['BTC_Volume'] = btc_full['Volume']
merged_df['BTC_Volume_Change'] = np.log(merged_df['BTC_Volume'] / merged_df['BTC_Volume'].shift(1))

# Calculate log returns for ALL tickers
for col in merged_df.columns:
    if col in tickers.values():
        merged_df[f'{col}_Log_Return'] = np.log(merged_df[col] / merged_df[col].shift(1))

merged_df = merged_df.dropna()
merged_df = merged_df.replace([np.inf, -np.inf], np.nan).dropna()

print(f"merged_df after log return calculation end date: {merged_df.index.max()}")
display(merged_df.tail())

# Correlations
print("\nCorrelation Matrix (Log Returns):")
log_return_cols = [c for c in merged_df.columns if 'Log_Return' in c]
display(merged_df[log_return_cols].corr())

# Graph: Linear Regression BTC vs SP500 Log Returns
plt.figure(figsize=(10, 6))
sns.regplot(x=merged_df['SP500_Log_Return'], y=merged_df['BTC_Log_Return'], scatter_kws={'alpha':0.3})
plt.title('GRAPH 4: Linear Regression of Bitcoin Log Return vs. SP500 Log Return')
plt.xlabel('SP500 Log Return')
plt.ylabel('Bitcoin Log Return')
plt.grid(True)
plt.show()

# Standardize Volatility
merged_df['rolling_std_SP500'] = merged_df['SP500_Log_Return'].rolling(window=30).std()
merged_df['rolling_std_BTC'] = merged_df['BTC_Log_Return'].rolling(window=30).std()

# RSI
def calculate_rsi(data, window=14):
    delta = data.diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
    rs = gain / loss
    return 100 - (100 / (1 + rs))

merged_df['RSI'] = calculate_rsi(merged_df['BTC'])

# calculate how far (%) the price is from the average.
sma_10 = merged_df['BTC'].rolling(window=10).mean()
sma_50 = merged_df['BTC'].rolling(window=50).mean()

merged_df['Dist_SMA_10'] = (merged_df['BTC'] / sma_10) - 1
merged_df['Dist_SMA_50'] = (merged_df['BTC'] / sma_50) - 1

merged_df['RSI_VIX_Ratio'] = merged_df['RSI'] / merged_df['VIX']

# Lags
merged_df['BTC_Return_Lag1'] = merged_df['BTC_Log_Return'].shift(1)
merged_df['BTC_Return_Lag2'] = merged_df['BTC_Log_Return'].shift(2)
merged_df['rolling_std_BTC_Lag1'] = merged_df['rolling_std_BTC'].shift(1)
merged_df['SP500_Return_Lag1'] = merged_df['SP500_Log_Return'].shift(1)
merged_df['VIX_Return_Lag1'] = merged_df['VIX_Log_Return'].shift(1)
merged_df['DXY_Return_Lag1'] = merged_df['DXY_Log_Return'].shift(1)
merged_df['TNX_Return_Lag1'] = merged_df['TNX_Log_Return'].shift(1)
merged_df['NASDAQ100_Log_Return_Lag1'] = merged_df['NASDAQ100_Log_Return'].shift(1)

merged_df.dropna(inplace=True)

#TARGET DEFINITION (NOISE FILTER)

# Calculate future return
merged_df['Next_Log_Return'] = merged_df['BTC_Log_Return'].shift(-1)

# Define Threshold (0.6% move required)
threshold = 0.006

# Create Target (1=Up, 0=Down, NaN=Flat)
merged_df['Target'] = np.where(merged_df['Next_Log_Return'] > threshold, 1,
                               np.where(merged_df['Next_Log_Return'] < -threshold, 0, np.nan))

# Drop "Flat" days to create a clean dataset
clean_df = merged_df.dropna(subset=['Target', 'Next_Log_Return']).copy()
clean_df['Target'] = clean_df['Target'].astype(int)

print(f"Original Days: {len(merged_df)}")
print(f"Cleaned Days (Significant Moves): {len(clean_df)}")

feature_cols = [
    'BTC_Log_Return', 'Dist_SMA_10', 'Dist_SMA_50', 'RSI', 'RSI_VIX_Ratio',
    'BTC_Return_Lag1', 'BTC_Return_Lag2',
    'SP500_Return_Lag1', 'rolling_std_SP500',
    'VIX_Log_Return', 'VIX_Return_Lag1',
    'DXY_Log_Return', 'TNX_Log_Return',
    'rolling_std_BTC', 'rolling_std_BTC_Lag1',
    'BTC_Volume_Change', 'NASDAQ100_Log_Return_Lag1'
]

print(f"Features used: {feature_cols}")

clean_df = clean_df.replace([np.inf, -np.inf], np.nan).dropna()
X = clean_df[feature_cols]
y = clean_df['Target']

# Double check that y is integer type (0 or 1)
y = y.astype(int)
# Split data: Train on the past, Test on the future (Shuffle=False is critical!)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)


# Machine Learning Model (The "Sniper" Approach)
print("\n--- Training Random Forest Model (Sniper Mode) ---")


rf_model = RandomForestClassifier(n_estimators=200, max_depth=5, min_samples_split=20, random_state=42)
rf_model.fit(X_train, y_train)

# B. Get Probabilities instead of just Predictions
# This gives us the % confidence for [Down, Up] (e.g., [0.40, 0.60])
probs = rf_model.predict_proba(X_test)

# C. Apply the "Sniper Threshold"
# We only count a prediction if the model is > 55% confident
confidence_threshold = 0.55

custom_preds = []
actuals = []
dates = []

print(f"\nScanning {len(X_test)} days for high-confidence setups...")

for i in range(len(probs)):
    # probs[i][0] is probability of DOWN
    # probs[i][1] is probability of UP
    prob_down = probs[i][0]
    prob_up = probs[i][1]

    if prob_up > confidence_threshold:
        custom_preds.append(1) # Predict UP
        actuals.append(y_test.iloc[i])
        dates.append(X_test.index[i])
    elif prob_down > confidence_threshold:
        custom_preds.append(0) # Predict DOWN
        actuals.append(y_test.iloc[i])
        dates.append(X_test.index[i])
    else:
        # Model is unsure (e.g., 51% vs 49%). We SKIP this day.
        pass

# Evaluate the "Sniper" Results
if len(custom_preds) > 0:
    results_df = pd.DataFrame({'Actual': actuals, 'Predicted': custom_preds}, index=dates)
    results_df['Correct'] = results_df['Actual'] == results_df['Predicted']

    accuracy = results_df['Correct'].mean()
    print(f"\n--- SNIPER MODEL RESULTS ---")
    print(f"Total Trades Taken: {len(results_df)} out of {len(X_test)} potential days")
    print(f"Skipped Days (Too Risky): {len(X_test) - len(results_df)}")
    print(f"Final Accuracy: {accuracy:.2%}")

    # Plot Cumulative Accuracy
    results_df['Win'] = results_df['Correct'].astype(int)
    results_df['Cumulative_Accuracy'] = results_df['Win'].expanding().mean()

    plt.figure(figsize=(12, 6))
    results_df['Cumulative_Accuracy'].plot(title=f'Sniper Model Accuracy (Threshold {confidence_threshold*100}%)')
    plt.axhline(y=0.5, color='r', linestyle='--', label='Random Guessing (50%)')
    plt.axhline(y=accuracy, color='g', linestyle=':', label=f'Final Acc: {accuracy:.2%}')
    plt.legend()
    plt.ylabel('Accuracy')
    plt.show()

else:
    print("Threshold too high! The model didn't take any trades.")
